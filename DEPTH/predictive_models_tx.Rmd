---
title: "Predicting models for outcomes"
author: "XZ"
date: "July 14, 2022"
output: html_document
---

```{r setup, include=F}
knitr::opts_chunk$set(root.dir=paste0(getwd(), '/DEPTH'))
```

## Load packages and dataset
```{r, message=F, warning=F}
library(caret)

setwd(paste0(getwd(), '/DEPTH'))
source('dataloader.R')
```

## Select needed variables
```{r}
dat <- dt[, .SD[1],
          client_epi,
          .SDcols=c('pt_posttest', 
                    'pt_pretest', 
                    'empathy',
                    'mispirit',
                    'ave_FA', 
                    'ave_GI', 
                    'ave_MIN', 
                    'ave_MIA', 
                    'ave_QUC', 
                    'ave_QUO', 
                    'ave_REC',
                    'ave_RES', 
                    'ave_ST'
                    )
          ][, -1]
```

## create a function that fits `caret` models
default setting:  
`validation method`: Repeated 10-fold cross-validation, repeated 5 times  
`search method`: grid search  
`loss function`: RMSE  
`iteration`: 50

```{r}
caretTrain <- function(DT, algorithm, y, iteration=50, loss_function='RMSE', search_method='grid', seed=123, ...) {
  
  r2s <- c()
  rmses <- c()
  
  set.seed(seed)
  
  ## iterative training to minimize random error introduced by train-test split
  i <- 1
  while(i <= iteration) {
    split_ind <- createDataPartition(DT[[y]], p=0.8, list=F)
    dat_train <- DT[split_ind]
    dat_test <- DT[!split_ind]
    
    ## set training controls and train
    fitControl <- trainControl(
      method='repeatedcv',
      number=10,
      repeats=5,
      search=search_method,
      selectionFunction='best',
      summaryFunction=defaultSummary
    )

    fit <- train(
      form=formula(sprintf('%s ~ .', y)),
      data=dat_train,
      method=algorithm,
      metric=loss_function,
      trControl=fitControl,
      ...
    )
    
    ## assess model performance
    preds <- predict(fit, dat_test[, .SD, .SDcols=!y])

    r2 <- R2(preds, dat_test[[y]])
    rmse <- RMSE(preds, dat_test[[y]])
  
    r2s <- c(r2s, r2)
    rmses <- c(rmses, rmse)

    i <- i + 1
  }
  
  ## report the metrics
  print('Mean R2:')
  print(mean(r2s, na.rm=T))
  print('Mean RMSE:')
  print(mean(rmses, na.rm=T))
  
  return(fit)
}
```

## get the baseline using simple linear regression  
To account for biases due to random data splitting, the entire process is repeated M times and taking the average.  
`M`: 1000

```{r, warning=F}
r2s <- c()
rmses <- c()

set.seed(123)

i <- 1
while(i <= 1000) {
  split_ind <- createDataPartition(dat$pt_posttest, p=0.8, list=F)
  dat_train <- dat[split_ind]
  dat_test <- dat[!split_ind]
  
  model_baseline_misc <- 
    lm(pt_posttest ~ .,
       data=dat_train[, .SD, .SDcols=!ncol(dat_train)]
    )
  
  preds <- predict(model_baseline_misc, dat_test[, .SD, .SDcols=!'pt_posttest'])
  r2 <- R2(preds, dat_test$pt_posttest)
  rmse <- RMSE(preds, dat_test$pt_posttest)

  r2s <- c(r2s, r2)
  rmses <- c(rmses, rmse)
  i <- i + 1
}

## report the metrics
print('Mean R2:')
mean(r2s, na.rm=T)
print('Mean RMSE:')
mean(rmses, na.rm=T)
```

## test SVM with Gaussian kernel
```{r}
model_svm_misc <- caretTrain(dat, 'svmRadial', 100)
```

## test LASSO
```{r}
model_lasso_misc <- caretTrain(dat, 'lasso', 'pt_posttest', 100)
```
[1] "Mean R2:"
[1] 0.4002157
[1] "Mean RMSE:"
[1] 0.5914227

## test elastic net
```{r}
model_enet_misc <- caretTrain(dat, 'enet', 100)
```

## test stochastic gradient boosting
```{r}
model_sgb_misc <- caretTrain(dat, 'gbm', 30)
```
[1] "Mean R2:"
[1] 0.3912287
[1] "Mean RMSE:"
[1] 0.5989364

## test extreme gradient boosting
```{r}
caretTrain(dat, 'xgbDART', 30)
```

## test k nearest neighbors
```{r}
caretTrain(dat, 'knn', 100)
```

## test random forest
```{r}
model_rf_misc <- caretTrain(dat, 'ranger', 30)
```

## test full connected neural networks
```{r}
model_mlp_misc <- caretTrain(dat, 'mlp', 30)
```
[1] "Mean R2:"
[1] 0.3322299
[1] "Mean RMSE:"
[1] 0.6793567

## test stacked autoencoder deep neural net
```{r}
model_dnn_misc <- caretTrain(dat, 'dnn', 10)
```
[1] "Mean R2:"
[1] 0.01432013
[1] "Mean RMSE:"
[1] 0.7839144
